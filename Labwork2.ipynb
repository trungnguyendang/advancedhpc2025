{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gg3TEvWFX8W",
        "outputId": "fa89bd79-98b5-4c03-b9a5-340b5d9ef031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba-cuda in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numba>=0.59.1 in /usr/local/lib/python3.12/dist-packages (from numba-cuda) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.59.1->numba-cuda) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.12/dist-packages (from numba>=0.59.1->numba-cuda) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba-cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "cc_cores_per_SM_dict = {\n",
        "    (2,0) : 32,\n",
        "    (2,1) : 48,\n",
        "    (3,0) : 192,\n",
        "    (3,5) : 192,\n",
        "    (3,7) : 192,\n",
        "    (5,0) : 128,\n",
        "    (5,2) : 128,\n",
        "    (6,0) : 64,\n",
        "    (6,1) : 128,\n",
        "    (7,0) : 64,\n",
        "    (7,5) : 64,\n",
        "    (8,0) : 64,\n",
        "    (8,6) : 128,\n",
        "    (8,9) : 128,\n",
        "    (9,0) : 128,\n",
        "    (10,0) : 128,\n",
        "    (12,0) : 128\n",
        "    }\n",
        "\n",
        "#check if there is a GPU available\n",
        "if cuda.is_available():\n",
        "    for i, gpu in enumerate(cuda.gpus):\n",
        "        with gpu:\n",
        "            device = cuda.get_current_device()\n",
        "            my_sms = getattr(device, 'MULTIPROCESSOR_COUNT')\n",
        "            my_cc = device.compute_capability\n",
        "            cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
        "            total_cores = cores_per_sm*my_sms\n",
        "            free_mem, total_mem = cuda.current_context().get_memory_info()\n",
        "            # ref answer from https://stackoverflow.com/questions/63823395/how-can-i-get-the-number-of-cuda-cores-in-my-gpu-using-python-and-numba\n",
        "            print(f\"=== GPU {i} ===\")\n",
        "            print(f\"Name: {device.name}\")\n",
        "            print(\"multiprocessor count: \", my_sms)\n",
        "            print(\"total cores: \", total_cores)\n",
        "            print(\"free memory: \", free_mem//(1024**2), \"MB\")\n",
        "            print(\"total memory: \", total_mem//(1024**2), \"MB\")\n",
        "else:\n",
        "    print(\"No GPU available\")"
      ],
      "metadata": {
        "id": "m98rNXPP72uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79732724-4e88-4699-fa2b-e2af16f0e84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU 0 ===\n",
            "Name: b'Tesla T4'\n",
            "multiprocessor count:  40\n",
            "total cores:  2560\n",
            "free memory:  14992 MB\n",
            "total memory:  15095 MB\n"
          ]
        }
      ]
    }
  ]
}